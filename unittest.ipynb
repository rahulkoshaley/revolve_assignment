{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_with_df (__main__.CheckNone) ... ok\n",
      "test_with_df (__main__.TestForCustomerCsv) ... ok\n",
      "test_with_df (__main__.TestForProductCsv) ... ok\n",
      "test_for_rdd (__main__.TestForSparkRDD) ... /home/unicorp/anaconda3/envs/env1/lib/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=50, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41588), raddr=('127.0.0.1', 45515)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_with_df (__main__.TestForTransactionJson) ... ok\n",
      "test (__main__.Test_Spark) ... /home/unicorp/anaconda3/envs/env1/lib/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=50, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 36292), raddr=('127.0.0.1', 40185)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 7.539s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f656fa139d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from unittest_pyspark import as_list, get_spark\n",
    "import pyspark.sql.types as pst\n",
    "\n",
    "class PySparkTestCase(unittest.TestCase):\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        self.spark= SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "       .appName(\"revolve\") \\\n",
    "       .getOrCreate()\n",
    "    def tearDown(self):\n",
    "        \"\"\"\n",
    "        Stop Spark\n",
    "        \"\"\"\n",
    "        self.spark.stop()\n",
    "\n",
    "\n",
    "class Test_Spark(PySparkTestCase):\n",
    "    \n",
    "\n",
    "    def test(self):\n",
    "        input = [ pst.Row(a=1, b=2)]\n",
    "        input_df = self.spark.createDataFrame(input)\n",
    "\n",
    "        expect = [{'a':1}]\n",
    "\n",
    "        actual_df = input_df.select(\"a\")\n",
    "        actual = as_list(actual_df)\n",
    "\n",
    "        self.assertEqual(actual, expect)\n",
    "\n",
    "\n",
    "class CheckNone(PySparkTestCase):\n",
    "    \n",
    "\n",
    "    def test_with_df(self):\n",
    "        df=  self.spark.read.options(header='True', inferSchema='True').csv(\"/home/unicorp/evolve/data/starter/customers.csv\")\n",
    "                                                                       #put path of customer csv file\n",
    "        df.createOrReplaceTempView(\"customer\")\n",
    "        df1=  self.spark.read.options(header='True', inferSchema='True').csv(\"/home/unicorp/evolve/data/starter/products.csv\")\n",
    "                                                                       #put path of customer csv file\n",
    "        df1.createOrReplaceTempView(\"product\")\n",
    "        null_for_customer= self.spark.sql(\"SELECT * FROM customer where customer_id IS NULL or loyalty_score IS NULL\")\n",
    "        null_for_product= self.spark.sql(\"SELECT * FROM product where product_id  IS NULL or product_description IS NULL or product_category IS NULL\")\n",
    "        message =\"None value present\"\n",
    "        self.assertEqual(null_for_customer.count(), 0,message)\n",
    "        self.assertEqual(null_for_product.count(), 0,message)\n",
    "\n",
    "\n",
    "class TestForCustomerCsv(PySparkTestCase):\n",
    "   \n",
    "\n",
    "    def test_with_df(self):\n",
    "        df=  self.spark.read.options(header='True', inferSchema='True').csv(\"/home/unicorp/evolve/data/starter/customers.csv\")\n",
    "                                                                       #put path of customer csv file\n",
    "        message = \"First value and second value are not equal !\"\n",
    "        self.assertEqual(df.count(), 137,message)\n",
    "\n",
    "class TestForProductCsv(PySparkTestCase):\n",
    "   \n",
    "\n",
    "    def test_with_df(self):\n",
    "        df=  self.spark.read.options(header='True', inferSchema='True').csv(\"/home/unicorp/evolve/data/starter/products.csv\")\n",
    "                                                                        #put path of product csv file\n",
    "        message = \"First value and second value are not equal !\"\n",
    "        self.assertEqual(df.count(), 64,message)\n",
    "        \n",
    "\n",
    "        \n",
    "class TestForTransactionJson(PySparkTestCase):\n",
    "   \n",
    "\n",
    "    def test_with_df(self):\n",
    "        df=  self.spark.read.options(header='True', inferSchema='True').json(\"/home/unicorp/evolve/data/starter/transactions/*/*.json\") \n",
    "                                                                        #put path of transaction json folder\n",
    "        \n",
    "        message = \"First value and second value are not equal !\"\n",
    "        self.assertEqual(df.count(), 2229,message)\n",
    "        \n",
    "class TestForSparkRDD(PySparkTestCase):       \n",
    "    \n",
    "\n",
    "\n",
    "    def test_for_rdd(self):\n",
    "        test_input = [\n",
    "            ' hello spark ',\n",
    "            ' hello again spark spark'\n",
    "        ]\n",
    "\n",
    "        input_rdd = self.spark.sparkContext.parallelize(test_input, 1)\n",
    "\n",
    "        from operator import add\n",
    "\n",
    "        results = input_rdd.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(add).collect()\n",
    "        self.assertEqual(results, [('hello', 2), ('spark', 3), ('again', 1)])\n",
    "        print(\"test done\")\n",
    "  \n",
    "    \n",
    "unittest.main(argv=[''],verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
